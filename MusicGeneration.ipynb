{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Music_Generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yogso/music-generation/blob/master/MusicGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "G7HYsy1xIllc",
        "colab_type": "code",
        "outputId": "8b89cbc7-6a4d-45f5-94a7-d2b974b2fc71",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1180
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Please upload the midi songs with which you want to feed the Machine Learning models { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "%cd /content\n",
        "\n",
        "!apt-get install libasound2-dev swig\n",
        "!pip install git+https://github.com/vishnubob/python-midi@feature/python3\n",
        "!wget https://raw.githubusercontent.com/yogso/creacion-musical-automatica/master/Red%20Convolucional/manipulacion_midi.py\n",
        "\n",
        "\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Reshape, Flatten, LSTM\n",
        "from keras.models import Model, Sequential\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.externals import joblib\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import manipulacion_midi\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers.core import Activation, Dropout\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "#Comprimir y subir las canciones que se usarán en el entrenamiento en un archivo que tenga por nombre \"canciones_entrenamiento.zip\"\n",
        "!rm -rf /content/canciones_entrenamiento || true\n",
        "\n",
        "%mkdir /content/canciones_entrenamiento\n",
        "%cd /content/canciones_entrenamiento\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "%cd /content\n",
        "\n",
        "#!unzip canciones_entrenamiento.zip -d /content/canciones_entrenamiento"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libasound2 libasound2-data swig3.0\n",
            "Suggested packages:\n",
            "  libasound2-plugins alsa-utils libasound2-doc swig-doc swig-examples\n",
            "  swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  libasound2 libasound2-data libasound2-dev swig swig3.0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 5 not upgraded.\n",
            "Need to get 1,619 kB of archives.\n",
            "After this operation, 8,444 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libasound2-data all 1.1.3-5ubuntu0.1 [36.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libasound2 amd64 1.1.3-5ubuntu0.1 [359 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libasound2-dev amd64 1.1.3-5ubuntu0.1 [123 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,619 kB in 8s (215 kB/s)\n",
            "Selecting previously unselected package libasound2-data.\n",
            "(Reading database ... 22280 files and directories currently installed.)\n",
            "Preparing to unpack .../libasound2-data_1.1.3-5ubuntu0.1_all.deb ...\n",
            "Unpacking libasound2-data (1.1.3-5ubuntu0.1) ...\n",
            "Selecting previously unselected package libasound2:amd64.\n",
            "Preparing to unpack .../libasound2_1.1.3-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking libasound2:amd64 (1.1.3-5ubuntu0.1) ...\n",
            "Selecting previously unselected package libasound2-dev:amd64.\n",
            "Preparing to unpack .../libasound2-dev_1.1.3-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking libasound2-dev:amd64 (1.1.3-5ubuntu0.1) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up libasound2-data (1.1.3-5ubuntu0.1) ...\n",
            "Setting up libasound2:amd64 (1.1.3-5ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libasound2-dev:amd64 (1.1.3-5ubuntu0.1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Collecting git+https://github.com/vishnubob/python-midi@feature/python3\n",
            "  Cloning https://github.com/vishnubob/python-midi (to revision feature/python3) to /tmp/pip-req-build-wd27ced8\n",
            "Branch 'feature/python3' set up to track remote branch 'feature/python3' from 'origin'.\n",
            "Switched to a new branch 'feature/python3'\n",
            "Building wheels for collected packages: midi\n",
            "  Running setup.py bdist_wheel for midi ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-egmyio0o/wheels/63/f9/4a/5e881f1126db389dfda75672c69b5be5bf51b0925cc7b5cbcf\n",
            "Successfully built midi\n",
            "Installing collected packages: midi\n",
            "Successfully installed midi-0.2.3\n",
            "--2018-11-11 17:05:56--  https://raw.githubusercontent.com/yogso/creacion-musical-automatica/master/Red%20Convolucional/manipulacion_midi.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3866 (3.8K) [text/plain]\n",
            "Saving to: ‘manipulacion_midi.py’\n",
            "\n",
            "manipulacion_midi.p 100%[===================>]   3.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-11-11 17:05:57 (58.2 MB/s) - ‘manipulacion_midi.py’ saved [3866/3866]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/canciones_entrenamiento\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9cda7bcc-97e1-4fdd-8cf4-d6bb7318a33a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9cda7bcc-97e1-4fdd-8cf4-d6bb7318a33a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xlcGXzqQgaPt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Model_dropdown = 'Convolutional_Network' #@param [\"Feedforward_Network\", \"Decision_Tree\", \"Random_Forest\", \"LSTM_Network\", \"Convolutional_Network\"]\n",
        "\n",
        "previous_times = 5 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "epoch = 300 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "longitud_minima=15                 #Tamaño minimo (en tiempos) que deben tener las canciones del conjunto de entrenamiento\n",
        "carpeta_canciones='canciones_entrenamiento'     #Carpeta donde estan las canciones el entrenamiento\n",
        "modelo_guardado='mejor_modelo.hdf5'  #Nombre del archivo donde se guardara el modelo entrenado\n",
        "\n",
        "#Elimina los tiempos vacios antes de todas las canciones\n",
        "def eliminar_zeros_iniciales(arr):\n",
        "  i=0\n",
        "  ceros=np.zeros_like(arr[0])\n",
        "  tamano=np.size(arr,0)\n",
        "  while np.array_equal(arr[i],ceros) and tamano!=1:\n",
        "    arr=np.delete(arr,i, axis=0)\n",
        "    tamano=tamano-1\n",
        "  return arr\n",
        "\n",
        "def leer_canciones(ruta):\n",
        "  print('Loading songs...')\n",
        "  archivos = glob.glob('{}/*.mid*'.format(ruta))\n",
        "  archivos = np.array(np.sort(archivos))\n",
        "  archivos = archivos.tolist()\n",
        "  global canciones\n",
        "  global numero_cancion\n",
        "  canciones = []\n",
        "  numero_cancion=0\n",
        "  for f in tqdm(archivos):\n",
        "    #print(f)\n",
        "    try:\n",
        "      cancion = np.array(manipulacion_midi.midiToNoteStateMatrix(f))\t\t\t#Convierte los archivos .mid a matrices \n",
        "      cancion=eliminar_zeros_iniciales(cancion)\n",
        "      tam = np.array(cancion).shape[0]\n",
        "      print(tam)\n",
        "      if tam > longitud_minima:\n",
        "        if(numero_cancion==0):\n",
        "          canciones=cancion\n",
        "          numero_cancion=numero_cancion+1\n",
        "        else:\n",
        "          canciones= np.concatenate([canciones, cancion], axis=0)\n",
        "          numero_cancion=numero_cancion+1\n",
        "    except Exception as e:\n",
        "      raise e\n",
        "  return canciones\n",
        "\n",
        "#Convierte el arreglo 2D de entradas en un arreglo 3D para entrenar el sistema\n",
        "def dimensionar(w):\n",
        "  for t, val in enumerate(w):\n",
        "    u[0, t] = val\n",
        "  return u\n",
        "\n",
        "\n",
        "  #Cargar canciones\n",
        "canciones = leer_canciones(carpeta_canciones)\n",
        "print(\"{} songs were loaded\".format(numero_cancion))\n",
        "    \n",
        "\n",
        "#Definir modelo a utilizar\n",
        "\n",
        "def FeedforwardNetworkImpl():\n",
        "  \n",
        "  #Dividir las canciones en dos arreglos, uno para los tiempos previos y otro para tiempo de salida  \n",
        "\n",
        "  sentences=[]\n",
        "  next_values=[]\n",
        "  for i in range(0, len(canciones) - previous_times):\n",
        "    sentences.append(canciones[i: i + previous_times])\n",
        "    next_values.append(canciones[i + previous_times])\n",
        "  \n",
        "  X = np.zeros((len(sentences), previous_times*156), dtype=np.int32)\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    X[i] = sentence.ravel() \n",
        "  y = np.asarray(next_values)\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Dense(400, activation='relu', input_dim=(previous_times*156)))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(400, activation='relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(400, activation='relu'))\n",
        "  model.add(Dense(156))\n",
        "  model.add(Activation('sigmoid'))\n",
        "  \n",
        "  model.summary()\n",
        "\n",
        "  #Define como se guarda el modelo\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam' , metrics=['binary_crossentropy'])\n",
        "  checkpoint = ModelCheckpoint(modelo_guardado, monitor='binary_crossentropy', verbose=1, save_best_only=True, mode='min')\n",
        "  callbacks_list = [checkpoint]\n",
        "\n",
        "  #Entrenar modelo\n",
        "\n",
        "  model.fit(X, y, epochs=epoch, batch_size=2000, callbacks=callbacks_list)\n",
        "\n",
        "  print (\"The model has been successfully trained\")\n",
        "\n",
        "  #Evaluar modelo\n",
        "  print(\"The error obtained with the model is: {}\". format(model.evaluate(X, y, batch_size=200, verbose=1)))\n",
        "  \n",
        "  return model\n",
        "\n",
        "  \n",
        "def DecisionTreeImpl():\n",
        "  \n",
        "  X=[]\n",
        "  y=[]\n",
        "  for i in range(0, len(canciones) - previous_times):\n",
        "    X.append(np.ravel(canciones[i: i + previous_times]))\n",
        "    y.append(canciones[i + previous_times])\n",
        "  \n",
        "  profundidad_arbol = 100\n",
        "\n",
        "  #Entrenamiento\n",
        "  model = tree.DecisionTreeClassifier(max_depth=profundidad_arbol)\n",
        "  model = model.fit(X, y)\n",
        "\n",
        "  print (\"The model has been successfully trained\")\n",
        "\n",
        "  #Evaluar modelo\n",
        "  print(\"The error obtained with the model is: {}\". format(log_loss(np.asarray(y), np.asarray(model.predict(X)))))\n",
        "  \n",
        "  return model\n",
        "\n",
        "  \n",
        "def RandomForestImpl():\n",
        "  \n",
        "  X=[]\n",
        "  y=[]\n",
        "  for i in range(0, len(canciones) - previous_times):\n",
        "    X.append(np.ravel(canciones[i: i + previous_times]))\n",
        "    y.append(canciones[i + previous_times])\n",
        "  \n",
        "  profundidad_arbol=200                     #Profundidad máxima de cada árbol individual\n",
        "  cantidad_arboles=15                       #Árboles que se crearan en el bosque\n",
        "\n",
        "  model = RandomForestClassifier(max_depth=profundidad_arbol, n_estimators=cantidad_arboles)\n",
        "  model = model.fit(X, y)\n",
        "\n",
        "  print (\"The model has been successfully trained\")\n",
        "\n",
        "  #Evaluar modelo\n",
        "  print(\"The error obtained with the model is: {}\". format(log_loss(np.asarray(y), np.asarray(model.predict(X)))))\n",
        "  \n",
        "  return model\n",
        "  \n",
        "\n",
        "def LSTMNetworkImpl():\n",
        "  \n",
        "  #Dividir las canciones en dos arreglos, uno para los tiempos previos y otro para tiempo de salida  \n",
        "\n",
        "  sentences=[]\n",
        "  next_values=[]\n",
        "  for i in range(0, len(canciones) - previous_times):\n",
        "    sentences.append(canciones[i: i + previous_times])\n",
        "    next_values.append(canciones[i + previous_times])\n",
        "    \n",
        "  #Convertir conjuntos de tiempos previos en un arreglo tridimensional \n",
        "\n",
        "  X = np.zeros((len(sentences), previous_times, 156), dtype=np.int32)\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    for t, val in enumerate(sentence):\n",
        "      X[i, t] = val\n",
        "  y = np.asarray(next_values)\n",
        "  \n",
        "  #Definir la red neuronal \n",
        "  \n",
        "  len(canciones)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(400, activation='relu', input_shape=(previous_times, 156), return_sequences=True))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(LSTM(400, activation='relu', return_sequences=True))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(LSTM(400, return_sequences=False))\n",
        "  model.add(Dense(156))\n",
        "  model.add(Activation('sigmoid'))\n",
        "\n",
        "  #Define como se guarda el modelo\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam' , metrics=['binary_crossentropy'])\n",
        "  checkpoint = ModelCheckpoint(modelo_guardado, monitor='binary_crossentropy', verbose=1, save_best_only=True, mode='min')\n",
        "  callbacks_list = [checkpoint]\n",
        "\n",
        "  #Entrenar modelo\n",
        "\n",
        "  model.fit(X, y, epochs=epoch, batch_size=200, callbacks=callbacks_list)\n",
        "\n",
        "  print (\"The model has been successfully trained\")\n",
        "\n",
        "  #Evaluar modelo\n",
        "  print(\"The error obtained with the model is: {}\". format(model.evaluate(X, y, batch_size=200, verbose=1)))\n",
        "  \n",
        "  return model\n",
        "  \n",
        "def ConvolutionalNetworkImpl(): \n",
        "  \n",
        "  #Dividir las canciones en dos arreglos, uno para los tiempos previos y otro para tiempo de salida  \n",
        "\n",
        "  sentences=[]\n",
        "  next_values=[]\n",
        "  for i in range(0, len(canciones) - previous_times):\n",
        "    sentences.append(canciones[i: i + previous_times])\n",
        "    next_values.append(canciones[i + previous_times])\n",
        "    \n",
        "  \n",
        "  #Convertir conjuntos de tiempos previos en un arreglo tridimensional \n",
        "\n",
        "  X = np.zeros((len(sentences), previous_times, 156), dtype=np.int32)\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    for t, val in enumerate(sentence):\n",
        "      X[i, t] = val\n",
        "  y = np.asarray(next_values)\n",
        "\n",
        "  #Definir la red neuronal \n",
        "  filtros=40\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Reshape((previous_times, 156, 1), input_shape=(previous_times, 156)))\n",
        "  model.add(Conv2D(kernel_size=4, filters=filtros, strides=1, padding='same', activation='relu', data_format='channels_last'))\n",
        "  model.add(Conv2D(kernel_size=4, filters=filtros, strides=1, padding='same', activation='relu', data_format='channels_last'))\n",
        "  model.add(Conv2D(kernel_size=4, filters=filtros, strides=1, padding='same', activation='relu', data_format='channels_last'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(600))\n",
        "  model.add(Dense(156))\n",
        "  model.add(Activation('sigmoid', name='sigmoid'))\n",
        "\n",
        "  #Define como se guarda el modelo\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam' , metrics=['categorical_crossentropy'])\n",
        "  checkpoint = ModelCheckpoint(modelo_guardado, monitor='binary_crossentropy', verbose=1, save_best_only=True, mode='min')\n",
        "  callbacks_list = [checkpoint]\n",
        "\n",
        "  #Entrenar modelo\n",
        "  model.fit(X, y, epochs=epoch, batch_size=1000, callbacks=callbacks_list)\n",
        "\n",
        "  print (\"The model has been successfully trained\")\n",
        "\n",
        "  #Evaluar modelo\n",
        "  print(\"The error obtained with the model is: {}\". format(model.evaluate(X, y, batch_size=200, verbose=1)))\n",
        "\n",
        "  return model\n",
        "\n",
        "op_models = {\"Feedforward_Network\" : FeedforwardNetworkImpl,\n",
        "           \"Decision_Tree\" : DecisionTreeImpl, \n",
        "           \"Random_Forest\" : RandomForestImpl, \n",
        "           \"LSTM_Network\" : LSTMNetworkImpl, \n",
        "           \"Convolutional_Network\" : ConvolutionalNetworkImpl\n",
        "          }\n",
        "\n",
        "func_model = op_models.get(Model_dropdown, 'Feedforward_Network')\n",
        "\n",
        "model = func_model()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxo3aAcUFtyG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls /content/canciones_generadas/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tyMCWRBPGeiC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title New songs generation\n",
        "\n",
        "num_songs = 10 #@param {type:\"integer\"}\n",
        "\n",
        "#Comprimir y subir las canciones que se usarán en el entrenamiento en un archivo que tenga por nombre \"canciones_entrenamiento.zip\"\n",
        "!rm -rf /content/canciones_generadas || true\n",
        "\n",
        "%mkdir /content/canciones_generadas\n",
        "\n",
        "if Model_dropdown == \"Decision_Tree\" or Model_dropdown == \"Random_Forest\":\n",
        "  \n",
        "  w = np.random.choice([0, 1], size=(previous_times,156),  p=[0.95,0.05])\n",
        "  plt.imshow(w)\n",
        "  \n",
        "    #Generar nueva cancion\n",
        "\n",
        "  tamano_cancion=200         #Numero de tiempos que tendra la cancion generada\n",
        "\n",
        "  \n",
        "  for num_song in tqdm(range (1,num_songs)):\n",
        "  \n",
        "    u = np.zeros((1, previous_times, 156), dtype=np.int32)\n",
        "    tamano_cancion=tamano_cancion+previous_times\n",
        "    cancion_base=w\n",
        "    ent=np.zeros((tamano_cancion,156))\n",
        "    ent[0:previous_times,:]=cancion_base\n",
        "    for i in tqdm(range (0,tamano_cancion-previous_times)):\t\t\t#Creacion de la cancion\n",
        "      nueva_fila = model.predict(np.reshape(ent[i:i+previous_times,:].ravel(),(1,previous_times*156)))\n",
        "      if (np.all(np.round_(nueva_fila)==0)):\n",
        "        nueva_fila[0,np.argsort(-nueva_fila)[0,:4]]=1\n",
        "      ent[i+previous_times] = nueva_fila\n",
        "      ent=np.round_(ent)\n",
        "\n",
        "    ent=ent[previous_times:tamano_cancion+previous_times,:]\n",
        "\n",
        "    #Guardar archivo midi\n",
        "    manipulacion_midi.noteStateMatrixToMidi(ent, \"/content/canciones_generadas/cancion_generada{}\".format(num_song))\n",
        "    \n",
        "    \n",
        "else:\n",
        "\n",
        "  #Generar aleatoriamente entrada inicial\n",
        "\n",
        "  w = np.random.choice([0, 1], size=(previous_times,156),  p=[0.95,0.05])\n",
        "  plt.imshow(w)\n",
        "\n",
        "  #Generar nueva cancion\n",
        "\n",
        "  tamano_cancion=200         #Numero de tiempos que tendra la cancion generada\n",
        "\n",
        "  for num_song in tqdm(range (1,num_songs)):\n",
        "\n",
        "    u = np.zeros((1, previous_times, 156), dtype=np.int32)\n",
        "    tamano_cancion=tamano_cancion+previous_times\n",
        "    cancion_base=w\n",
        "    ent=np.zeros((tamano_cancion,156))\n",
        "    ent[0:previous_times,:]=cancion_base\n",
        "    for i in range (0,tamano_cancion-previous_times):\t\t\t#Creacion de la cancion\n",
        "      nueva_fila = model.predict(dimensionar(ent[i:i+previous_times,:]), batch_size=1, verbose=1)\n",
        "      if (np.all(np.round_(nueva_fila)==0)):\n",
        "        nueva_fila[0,np.argsort(-nueva_fila)[0,:4]]=1\n",
        "      ent[i+previous_times] = nueva_fila\n",
        "      ent=np.round_(ent)\n",
        "\n",
        "    ent=ent[previous_times:tamano_cancion+previous_times,:]\n",
        "\n",
        "    #Guardar archivo midi\n",
        "    manipulacion_midi.noteStateMatrixToMidi(ent, \"/content/canciones_generadas/cancion_generada{}\".format(num_song))\n",
        "    \n",
        "    \n",
        "for f in range (1,num_songs):\n",
        "  files.download(\"/content/canciones_generadas/cancion_generada{}.mid\".format(f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tg8V2FDYrt7t",
        "colab_type": "code",
        "outputId": "e87f80f1-1ca6-430e-e951-c5492fb1684e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "canciones_entrenamiento  manipulacion_midi.py  __pycache__  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}